---
title: "Gradient Boosting"
author: "Shale"
date: "2/15/2022"
output: html_document
---

**Currently just taking sections of code from the feature selection .Rmd and trying the same stuff with Gradient Boosting to try and identify differences**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)

## Machine Learning packages
library(caret)
library(xgboost)
library(Ckmeans.1d.dp)

# XGBoost visualization dependencies
library(caTools)
library(DiagrammeR)

options(scipen = 5)
```

# Data Preparation

## Load Data

```{r load_data, message=FALSE}
df <- read_csv(here("data", "sageres.csv")) 
```

The code chunk below identifies the variables in our data set that will be converted to factors, variables that will be used to aggregate by water year, and the desired response variable.

```{r user_inputs}
## USER INPUTS
group_cols <- c("wy", "stratumID", "clim", "scen", "topo")
factor_vars <- c("wy", "stratumID", "clim", "scen", "topo")
response_var <- "npp"
```

```{r prepare_data}
## Convert categorical variables to factors
df[,factor_vars] <- lapply(df[,factor_vars], factor)

df_wy <- df %>% 
  ## Change aspect and splope from radians to degrees
  mutate(aspect=aspect*(180/pi),
         slope=slope*(180/pi)) %>% 
  group_by(across(all_of(group_cols))) %>% 
  mutate(jun_tavg = mean(tavg[month == 6]),
         jul_tavg = mean(tavg[month == 7]),
         aug_tavg = mean(tavg[month == 8]),
         sep_tavg = mean(tavg[month == 9]),
         oct_tavg = mean(tavg[month == 10]),
         nov_tavg = mean(tavg[month == 11]),
         dec_tavg = mean(tavg[month == 12]),
         jan_tavg = mean(tavg[month == 1]),
         feb_tavg = mean(tavg[month == 2]),
         mar_tavg = mean(tavg[month == 3]),
         apr_tavg = mean(tavg[month == 4]),
         may_tavg = mean(tavg[month == 5]),) %>% 
  mutate(peak_swe=max(swe)) %>%
  mutate(swe_precip_ratio=peak_swe/sum(precip)) %>% 
  summarise_if(is.numeric, mean) %>% 
  ungroup() %>% 
  ## Reorder response variables first
  select(!!response_var, everything()) %>% 
  ## Remove unwanted variables (manually?)
  select(-c(day, month, year, basinID, hillID, zoneID, patchID, wy))

## Create data frame with only climate scenario 0
df_wy0 <- df_wy %>% 
  filter(clim==0) %>% 
  select(-clim)

## Create data frame with only climate scenario 2
df_wy2 <- df_wy %>% 
  filter(clim==2) %>% 
  select(-clim)
```

The `xgboost` package requires a numeric matrix as input, so factor variables (`topo`, `scen`, `stratumID`) need to be converted to numeric:

```{r matrix_setup}
set.seed(10)

npp_lab <- df_wy$npp

sample_split <- sample.split(Y = df_wy$npp, SplitRatio = 0.8)
train_set <- subset(x = df_wy, sample_split == TRUE)
test_set <- subset(x = df_wy, sample_split == FALSE)

# Specify outcome variable (supervised learning)
npp_train <- train_set$npp
npp_test <- test_set$npp

## Convert factors to numeric for training data
xgb_matrix_train_x <- train_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix() %>% 
  xgb.DMatrix(label = npp_train)

## Convert factors to numeric for testing data
xgb_matrix_test_x <- test_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix() %>% 
  xgb.DMatrix(label = npp_test)

```

## Gradient Boosting Model

### Specify hyperparameters

Detailed description of parameters:
https://xgboost.readthedocs.io/en/stable/parameter.html

Parameter tuning guidelines:
https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html
**This document begins with the sentence: "Parameter tuning is a dark art in machine learning."** 
In Python: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/
From Kaggle: https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning

```{r}
xgb_params <- list(
  booster = "gbtree",
  eta = 0.03,
  max_depth = 10,
  gamma = 2,
  subsample = 0.8,
  colsample_bytree = 1,
  objective = "reg:squarederror",
  eval_metric = "rmse"
)
```

```{r}
xgb_model <- xgb.train(
#  params = xgb_params,
  data = xgb_matrix_train_x,
  nrounds = 5000,
  verbose = 1
)
xgb_model
```

```{r}
xgb_preds <- as.data.frame(predict(xgb_model, xgb_matrix_test_x, reshape = TRUE))
xgb_pred_df <- xgb_preds %>% 
  mutate(RHESSYS_npp = npp_test)
xgb_preds
```
# Visualize

```{r}
xgb_imp <- xgb.importance(model = xgb_model)

xgb.ggplot.importance(xgb_imp, xlab = "Relative importance")

```

```{r}
train_data <- train_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix()

xgb.plot.shap.summary(data = train_data, model = xgb_model)
```
```{r}
xgb.plot.multi.trees(model = xgb_model, features_keep = 10)


```

