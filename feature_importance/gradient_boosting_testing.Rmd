---
title: "Gradient Boosting"
author: "Shale"
date: "2/15/2022"
output: 
  rmarkdown::html_document:
    theme: cerulean
---

**Currently just taking sections of code from the feature selection .Rmd and trying the same stuff with Gradient Boosting to try and identify differences**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)

## Machine Learning packages
library(caret)
library(xgboost)
library(Ckmeans.1d.dp)

# XGBoost visualization dependencies
library(caTools)
library(DiagrammeR)

options(scipen = 5)
set.seed(101)
```

# Data Preparation

## Load Data

```{r load_data, message=FALSE}
df <- read_csv(here("data", "sageres.csv")) 
```

The code chunk below identifies the variables in our data set that will be converted to factors, variables that will be used to aggregate by water year, and the desired response variable.

```{r user_inputs}
## USER INPUTS
group_cols <- c("wy", "stratumID", "clim", "scen", "topo")
factor_vars <- c("wy", "stratumID", "clim", "scen", "topo")
response_var <- "npp"
```

```{r prepare_data}
## Convert categorical variables to factors
df[,factor_vars] <- lapply(df[,factor_vars], factor)

df_wy <- df %>% 
  ## Change aspect and splope from radians to degrees
  mutate(aspect=aspect*(180/pi),
         slope=slope*(180/pi)) %>% 
  group_by(across(all_of(group_cols))) %>% 
  mutate(jun_tavg = mean(tavg[month == 6]),
         jul_tavg = mean(tavg[month == 7]),
         aug_tavg = mean(tavg[month == 8]),
         sep_tavg = mean(tavg[month == 9]),
         oct_tavg = mean(tavg[month == 10]),
         nov_tavg = mean(tavg[month == 11]),
         dec_tavg = mean(tavg[month == 12]),
         jan_tavg = mean(tavg[month == 1]),
         feb_tavg = mean(tavg[month == 2]),
         mar_tavg = mean(tavg[month == 3]),
         apr_tavg = mean(tavg[month == 4]),
         may_tavg = mean(tavg[month == 5]),) %>% 
  mutate(peak_swe=max(swe)) %>%
  mutate(swe_precip_ratio=peak_swe/sum(precip)) %>% 
  summarise_if(is.numeric, mean) %>% 
  ungroup() %>% 
  ## Reorder response variables first
  select(!!response_var, everything()) %>% 
  ## Remove unwanted variables (manually?)
  select(-c(day, month, year, basinID, hillID, zoneID, patchID, wy))

## Create data frame with only climate scenario 0
df_wy0 <- df_wy %>% 
  filter(clim==0) %>% 
  select(-clim)

## Create data frame with only climate scenario 2
df_wy2 <- df_wy %>% 
  filter(clim==2) %>% 
  select(-clim)
```

The `xgboost` package requires a numeric matrix as input, so factor variables (`topo`, `scen`, `stratumID`) need to be converted to numeric:

```{r matrix_setup}

npp_lab <- df_wy$npp

sample_split <- sample.split(Y = df_wy$npp, SplitRatio = 0.8)
train_set <- subset(x = df_wy, sample_split == TRUE)
test_set <- subset(x = df_wy, sample_split == FALSE)

# Specify outcome variable (supervised learning)
npp_train <- train_set$npp
npp_test <- test_set$npp

## Convert factors to numeric for training data
xgb_matrix_train_x <- train_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix() %>% 
  xgb.DMatrix(label = npp_train)

## Convert factors to numeric for testing data
xgb_matrix_test_x <- test_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix() %>% 
  xgb.DMatrix(label = npp_test)

```


# Gradient Boosting Model

### Specify hyperparameters 

Detailed description of parameters: 
https://xgboost.readthedocs.io/en/stable/parameter.html

Parameter tuning guidelines: 
https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html 
**This document begins with the sentence: "Parameter tuning is a dark art in machine learning."** 
In Python: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ 
From Kaggle: https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning 

```{r}
xgb_params <- list(
  booster = "gbtree",
  eta = 0.03,
  max_depth = 10,
  gamma = 2,
  subsample = 0.8,
  colsample_bytree = 1,
  objective = "reg:squarederror",
  eval_metric = "rmse"
)
```

```{r}
xgb_model <- xgb.train(
#  params = xgb_params,
  data = xgb_matrix_train_x,
  nrounds = 5000,
  verbose = 1
)
xgb_model
```

```{r}
xgb_preds <- as.data.frame(predict(xgb_model, xgb_matrix_test_x, reshape = TRUE))
xgb_pred_df <- xgb_preds %>% 
  mutate(RHESSYS_npp = npp_test)

```
# Visualize

## XGBoost Package
```{r}
xgb_imp <- xgb.importance(model = xgb_model)

xgb.ggplot.importance(xgb_imp, xlab = "Relative importance")

```

```{r}
train_data <- train_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix()

xgb.plot.shap.summary(data = train_data, model = xgb_model)
```

```{r}
xgb.plot.multi.trees(model = xgb_model, features_keep = 10)


```

# Caret Package

```{r Cross-validation, message=FALSE, warning=FALSE}
# some shenanigans to get the seed into the right shape to work with 10-fold cross-validation
s = 4326
seed = list()

for (i in 1:10) {
  seed[[i]] <- rep(s, 36)
}

seed[[11]] = 4326

# Using the caret package to do 10-fold cross-validation
train_control = caret::trainControl(method = "cv", number = 10, seed = seed)

gb_model0_caret = train(npp~., data = df_wy0, method = "xgbTree", trControl = train_control)
gb_model2_caret = train(npp~., data = df_wy2, method = "xgbTree", trControl = train_control)
```

We can look at the optimized tuning parameters using the `bestTune` feature of the `train()` function:

 - `nrounds` (# Boosting Iterations)
 - `max_depth` (Max Tree Depth)
 - `eta` (Shrinkage)
 - `gamma` (Minimum Loss Reduction)
 - `colsample_bytree` (Subsample Ratio of Columns)
 - `min_child_weight` (Minimum Sum of Instance Weight)
 - `subsample` (Subsample Percentage)

```{r}
params0 = gb_model0_caret$bestTune
params2 = gb_model2_caret$bestTune
```

In the interest of efficiency, we can look at how long it took each model to train:

```{r}
# Random Forest models:

# Gradient Boosting Models:
gb_model0_caret$times
gb_model2_caret$times
```

Find variable importance:

```{r}
c0_imp = varImp(gb_model0_caret, scale = FALSE)
list_imp0 = c0_imp$importance
list_imp0 = list_imp0 %>% mutate(variable = row.names(list_imp0)) %>% 
  filter(list_imp0$Overall != 0)

c2_imp = varImp(gb_model0_caret, scale = FALSE)
list_imp2 = c2_imp$importance
list_imp2 = list_imp2 %>% mutate(variable = row.names(list_imp2)) %>% 
  filter(list_imp2$Overall != 0)
```


```{r}
ggplot(data = list_imp0, aes(x=Overall, y=variable)) + geom_col()
```

Can see that we are getting different top 3 between `xgboost` and `caret`, even within gradient boosting. Need to check stuff like defaults (gamma value? dealing with categoricals is clearly different? Also did 10-fold with caret but not with xgb.)

```{r}
plot(c_imp, top = 15)
```

