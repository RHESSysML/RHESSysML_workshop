---
title: "Gradient Boosting"
author: "Shale"
date: "2/15/2022"
output: 
  rmarkdown::html_document:
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)

## Machine Learning packages
library(caret)
library(xgboost)
library(Ckmeans.1d.dp)
library(mltools)

# XGBoost visualization dependencies
library(caTools)
#library(DiagrammeR)

options(scipen = 5)
```

# Data Preparation

adaboost is the earliest form of gradient boosting, loss function is hard-coded so less flexible:
https://www.analyticsvidhya.com/blog/2020/10/adaboost-and-gradient-boost-comparitive-study-between-2-popular-ensemble-model-techniques/
https://datascience.stackexchange.com/questions/39193/adaboost-vs-gradient-boosting

## GBM/XGB

Testing different implementations of `gbm` and `xgboost` methods through `caret`

All gradient boosting methods require one-hot encoding for factor variables:

```{r}
df_wy <- read_csv(here("shiny", "aggregated_datasets", "df_wy.csv"))

# Training vs response
y = df_wy$npp
x = df_wy[-1]

group_cols <- c("wy", "stratumID", "clim", "scen", "topo")
factor_vars <- c("wy", "stratumID", "clim", "scen", "topo")
response_var <- "npp"

x[,factor_vars] <- lapply(x[,factor_vars], factor)

xhot = x %>% data.table::data.table() %>% mltools::one_hot(cols = factor_vars)
```

https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/
stochastic gradient boosting helps avoid overfitting with `gbm`: https://www.sciencedirect.com/science/article/pii/S0167947301000652

## Parameter Tuning

There is a bit more complexity that goes into tuning a Gradient Boosting model than a Random Forest. The `expand.grid()` function will allow us to test multiple values of each parameter to optimize model performance. Some of the standard hyperparameters that we will tune here are:

 - `nrounds` (# Boosting Iterations)
 - `max_depth` (Max Tree Depth)
 - `eta` (Shrinkage)
 - `gamma` (Minimum Loss Reduction)
 - `colsample_bytree` (Subsample Ratio of Columns)
 - `min_child_weight` (Minimum Sum of Instance Weight)
 - `subsample` (Subsample Percentage)
 
Out of all these parameters, all except `nrounds` are used to prevent overfitting, which is a more common problem in boosting models than RFs. The inherent randomness of RFs prevents overfitting relatively effectively, but more care is required with GB. For example, the `colsample_bytree` parameter mimics the functionality of a RF by randomly sampling the specified proportions of columns to build a boosted tree.

Additionally, we will use `trainControl()` to implement 10-fold cross-validation to further avoid overfitting.

```{r}

fitControl <- caret::trainControl(
  method = "cv",
  number = 10,
  savePredictions = 'final'
)

tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 800, by = 50),
  eta = c(0.01, 0.02, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6), 
  gamma = c(0, 10, 50),
  colsample_bytree = c(.5, .7, .8, 1),
  min_child_weight = 1,
  subsample = c(.5, .7, .8, 1))

# different names for `gbm`
gbm_grid <- expand.grid(
  n.trees = seq(from = 100, to = 800, by = 50),
  shrinkage = c(0.01, 0.02, 0.05, 0.1, 0.3),
  interaction.depth = c(2, 3, 4, 5, 6), 
  n.minobsinnode = c(10, 30, 50))

```

-------------------------------------------
full cv, gbm


```{r}
set.seed(4326)

gbm_model_cv <- caret::train(
  x = xhot,
  y = y,
  trControl = fitControl,
  tuneGrid = gbm_grid,
  metric = 'RMSE',
  method = "gbm",
  verbose = FALSE
)
```

----------------------------------------------
full cv, xgboost
```{r}
install.packages("tictoc")
library(tictoc)

tic()
xgb_model_cv = train(x=xhot,
                      y=y,
                      method = "xgbTree",
                      trControl = fitControl,
                      tuneGrid = tune_grid,
                      verbosity = 0)
toc()

plot(xgb_model_cv)

# helper function for the plots
# tuneplot <- function(x, probs = .90) {
#   ggplot(x) +
#     coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
#     theme_bw()
# }
# 
# tuneplot(xgb_tune)
# from https://www.kaggle.com/code/pelkoja/visual-xgboost-tuning-with-caret/report
```
These models take a **LONG** time to train because of the number of hyperparameter combinations being tested - in effect, the `xgb_model_cv` object is actually about 19,000 separate boosted models, with different hyperparameters defined through `expand.grid()`. We suggest you **NOT** run them yourself! Instead, we have saved these models as objects in the `models` subdirectory of this project - the trained models can be quickly loaded from there to perform the necessary comparisons. 

```{r}
# save the models (one time only)
saveRDS(gbm_model_cv, here("models/GBm.rds"))
saveRDS(xgb_model_cv, here("models/xGB.rds"))

# read in models from file (all future access)
gbm_model_cv = readRDS(here("models/GBm.rds"))
xgb_model_cv = readRDS(here("models/xGB.rds"))
```


Comparison of prediction results

```{r}
predict(gbm_model_cv, xhot)
predict(xgb_model_cv, xhot)
```

```{r}
gbm_model_cv$results$RMSE
gbm_model_cv$results$Rsquared

xgb_model_cv$results$RMSE
xgb_model_cv$results$Rsquared
```


We can look at the optimized tuning parameters using the `bestTune` feature of the `train()` function:

```{r}
params0 = xgb_model_cv$bestTune
params2 = gbm_model_cv$bestTune
```


Find variable importance:

```{r}
# might try with `gbm` instead of `xgb` to be compatible with varImp()?

xgb_imp = varImp(xgb_model_cv, scale = FALSE)
list_impx = xgb_impx$importance
list_impx = list_impx %>% mutate(Variable = row.names(list_impx)) %>% 
  filter(list_impx$Overall != 0)

gbm_imp = varImp(gbm_model_cv, scale = FALSE)
list_imp = gbm_imp$importance
list_imp = list_imp %>% mutate(Variable = row.names(list_imp)) %>% 
  filter(list_imp$Overall != 0)

```


```{r}
# default plotting function
plot(xgb_imp, top = 15)
plot(gbm_imp, top = 15)


# using plotting function from rf workflow
xgb_plot <- plot_imp(list_impx)
gbm_plot <- plot_imp(list_imp)

wy0_plot + wy2_plot 

```

```{r}

```

