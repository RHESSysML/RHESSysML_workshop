---
title: "Gradient Boosting"
author: "Shale"
date: "2/15/2022"
output: 
  rmarkdown::html_document:
    theme: cerulean
---

**Currently just taking sections of code from the feature selection .Rmd and trying the same stuff with Gradient Boosting to try and identify differences**

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)

## Machine Learning packages
library(caret)
library(xgboost)
library(Ckmeans.1d.dp)

# XGBoost visualization dependencies
library(caTools)
library(DiagrammeR)

options(scipen = 5)
```

# Data Preparation

## Load Data

```{r load_data, message=FALSE}
df_wy <- read_csv(here("shiny", "aggregated_datasets", "df_wy.csv")) 

## Create data frame with only climate scenario 0
df_wy0 <- df_wy %>% 
  filter(clim==0) %>% 
  select(-clim)

## Create data frame with only climate scenario 2
df_wy2 <- df_wy %>% 
  filter(clim==2) %>% 
  select(-clim)
```

The `xgboost` package requires a numeric matrix as input, so factor variables (`topo`, `scen`, `stratumID`) need to be converted to numeric:

```{r matrix_setup}

npp_lab <- df_wy$npp

sample_split <- sample.split(Y = df_wy$npp, SplitRatio = 0.8)
train_set <- subset(x = df_wy, sample_split == TRUE)
test_set <- subset(x = df_wy, sample_split == FALSE)

# Specify outcome variable (supervised learning)
npp_train <- train_set$npp
npp_test <- test_set$npp

## Convert factors to numeric for training data
xgb_matrix_train_x <- train_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix() %>% 
  xgb.DMatrix(label = npp_train)

## Convert factors to numeric for testing data
xgb_matrix_test_x <- test_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix() %>% 
  xgb.DMatrix(label = npp_test)

```


# Gradient Boosting Model

### Specify hyperparameters 

Detailed description of parameters: 
https://xgboost.readthedocs.io/en/stable/parameter.html

Parameter tuning guidelines: 
https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html 
**This document begins with the sentence: "Parameter tuning is a dark art in machine learning."** 
In Python: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ 
From Kaggle: https://www.kaggle.com/prashant111/a-guide-on-xgboost-hyperparameters-tuning 

```{r}
xgb_params <- list(
  booster = "gbtree",
  eta = 0.03,
  max_depth = 10,
  gamma = 2,
  subsample = 0.8,
  colsample_bytree = 1,
  objective = "reg:squarederror",
  eval_metric = "rmse"
)
```

```{r}
xgb_model <- xgb.train(
#  params = xgb_params,
  data = xgb_matrix_train_x,
  nrounds = 5000,
  verbose = 1
)
xgb_model
```

```{r}
xgb_preds <- as.data.frame(predict(xgb_model, xgb_matrix_test_x, reshape = TRUE))
xgb_pred_df <- xgb_preds %>% 
  mutate(RHESSYS_npp = npp_test)

```
# Visualize

## XGBoost Package
```{r}
xgb_imp <- xgb.importance(model = xgb_model)

xgb.ggplot.importance(xgb_imp, xlab = "Relative importance")

```

```{r}
train_data <- train_set %>% 
  mutate(stratumID = as.numeric(as.character(stratumID)),
         scen = as.numeric(as.character(scen)),
         topo = as.numeric(topo),
         clim = as.numeric(clim)) %>%
  select(-npp) %>% 
  as.matrix()

xgb.plot.shap.summary(data = train_data, model = xgb_model)
```

```{r}
xgb.plot.multi.trees(model = xgb_model, features_keep = 10)


```


# Caret Package

## Caret/XGB

```{r Cross-validation, message=FALSE, warning=FALSE}
# setup for prediction
y = df_wy$response
x = df_wy[-1]

# some shenanigans to get the seed into the right shape to work with 10-fold cross-validation
# s = 4326
# seed = list()
# for (i in 1:10) {
#   seed[[i]] <- rep(s, 36)
# }
# seed[[11]] = 4326

fitControl <- caret::trainControl(
  method = "cv",
  number = 10,
  savePredictions = 'final'
)

xgbGrid <- expand.grid(
  nrounds = seq(from = 400, to = 600, 30),
  eta = c(0.01),
  max_depth = c(2:4),
  gamma = 0,
  colsample_bytree = c(0.8),
  min_child_weight = c(1, 5),
  subsample = c(1.0)
)

gb_mod_caret = train(x = x,
                     y = y,
                     method = "xgbTree",
                     trControl = fitControl,
                     tuneGrid = xgbGrid,
                     verbose = FALSE)

# gb_model0_caret = train(response~.,
#                         data = df_wy0,
#                         method = "xgbTree",
#                         trControl = train_control)
# gb_model2_caret = train(response~.,
#                         data = df_wy2,
#                         method = "xgbTree",
#                         trControl = train_control)
```

## Caret/ADA

https://www.analyticsvidhya.com/blog/2020/10/adaboost-and-gradient-boost-comparitive-study-between-2-popular-ensemble-model-techniques/

https://datascience.stackexchange.com/questions/39193/adaboost-vs-gradient-boosting

```{r}
# Training vs response
y = df_wy$response
x = df_wy[-1]

group_cols <- c("wy", "stratumID", "clim", "scen", "topo")
factor_vars <- c("wy", "stratumID", "clim", "scen", "topo")
response_var <- "npp"

x[,factor_vars] <- lapply(x[,factor_vars], factor)

xhot = x %>% data.table::data.table() %>% mltools::one_hot(cols = factor_vars)
xnum = x %>% select(!factor_vars)
```


```{r}
# different boosting package

set.seed(4326)
s = 4326

fitControl <- caret::trainControl(
  method = "cv",
  number = 10,
  savePredictions = 'final'
)

fitAdaboost <- caret::train(
 # response ~ .,
  x = xnum,
  y = y,
  data = df_wy,
  trControl = fitControl,
  tuneLength = 3,
  metric = 'RMSE',
  method = "adaboost",
  verbose = FALSE
)

gb_model0_ada = train(response~.,
                        data = df_wy0,
                        method = "bstTree",
                        trControl = fitControl,
                        verbosity = FALSE)
gb_model2_ada = train(x=xhot,
                      y=y,
                   #     data = df_wy2,
                        method = "xgbTree",
                        trControl = fitControl,
                        verbosity = 0)



# helper function for the plots
# tuneplot <- function(x, probs = .90) {
#   ggplot(x) +
#     coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
#     theme_bw()
# }
# 
# tuneplot(xgb_tune)
# from https://www.kaggle.com/code/pelkoja/visual-xgboost-tuning-with-caret/report
```


We can look at the optimized tuning parameters using the `bestTune` feature of the `train()` function:

 - `nrounds` (# Boosting Iterations)
 - `max_depth` (Max Tree Depth)
 - `eta` (Shrinkage)
 - `gamma` (Minimum Loss Reduction)
 - `colsample_bytree` (Subsample Ratio of Columns)
 - `min_child_weight` (Minimum Sum of Instance Weight)
 - `subsample` (Subsample Percentage)

```{r}
params0 = gb_model0_caret$bestTune
params2 = gb_model2_caret$bestTune
```

In the interest of efficiency, we can look at how long it took each model to train:

```{r}
# Random Forest models:

# Gradient Boosting Models:
gb_model0_caret$times
gb_model2_caret$times
```

Find variable importance:

```{r}
# might try with `gbm` instead of `xgb` to be compatible with varImp()?


c0_imp = varImp(gb_model0_caret, scale = FALSE)
list_imp0 = c0_imp$importance
list_imp0 = list_imp0 %>% mutate(Variable = row.names(list_imp0)) %>% 
  filter(list_imp0$Overall != 0)

c2_imp = varImp(gb_model0_caret, scale = FALSE)
list_imp2 = c2_imp$importance
list_imp2 = list_imp2 %>% mutate(Variable = row.names(list_imp2)) %>% 
  filter(list_imp2$Overall != 0)
```


```{r}
# using plotting function from rf workflow
wy0_plot <- plot_imp(list_imp0)
wy2_plot <- plot_imp(list_imp2)

wy0_plot + wy2_plot 
```

Can see that we are getting different top 3 between `xgboost` and `caret`, even within gradient boosting. Need to check stuff like defaults (gamma value? dealing with categoricals is clearly different? Also did 10-fold with caret but not with xgb.)

```{r}
plot(c_imp, top = 15)
```

