---
title: "Feature Selection Framework"
author: "RHESSysML Capstone Group"
date: "2/5/2022"
output: 
  rmarkdown::html_document:
    theme: cerulean
---

# Introduction

The following R Markdown document describes the necessary steps to determine important relationships between predictor variables and a response variable in RHESSys model output. Specific code examples will be based on RHESSys model output from the Sagehen Creek Experimental Watershed in the Sierra Nevada, CA. The data set incorporates model parameter uncertainty, topographic spatial variability, and climate change scenarios. The data set and associated metadata can be accessed here: <https://www.hydroshare.org/resource/2a31bd57b7e74c758b7857679ffbb4c5/>.

The following research question will be addressed in this process: **What are the most important predictors of Net Primary Productivity in differing climate scenarios?**

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen=999)

# Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)
library(zeallot)

# Machine Learning packages
library(caret)
library(spatialRF)
library(randomForest)
library(party)
library(partykit)
library(permimp)
library(rfUtilities)
library(randomForestExplainer)
```

# Data Preparation

## Load Data

```{r load_data, message=FALSE}
df <- read_csv(here("data", "sageres.csv")) 
```

## Clean and Aggregate Data

First, RHESSys output will need to be aggregated and cleaned based on specific research question. Possible changes include:

1.  Changing the temporal resolution (i.e. Daily to Yearly measurements).

2.  Converting variable units (i.e. Radians to Degrees).

3.  Converting variable class (i.e. numeric/character to factor).

4.  Creating derived variables (i.e. Peak SWE).

The code chunk below identifies the variables in our data set that will be converted to factors, variables that will be used to aggregate by water year, and the desired response variable.

```{r user_inputs}
group_cols <- c("wy", "stratumID", "clim", "scen", "topo")
factor_vars <- c("wy", "stratumID", "clim", "scen", "topo")
response_var <- "npp"
```

Next, the necessary modifications to the data set are made. For this example, we are converting factor variables, aggregating by water year, changing `slope` and `aspect` from radians to degrees, and adding two derived variables: `peak_swe` and `swe_precip_ratio`. Since this data set contains two climate scenarios, we split these up into two data frames.

```{r prepare_data}
# Convert categorical variables to factors
df[,factor_vars] <- lapply(df[,factor_vars], factor)

# Change aspect and slope from radians to degrees
df_wy <- df %>% 
  mutate(aspect=aspect*(180/pi),
         slope=slope*(180/pi))

# Create features for each monthly average temp.
df_wy <- df_wy %>%   
  group_by(across(all_of(group_cols))) %>% 
  mutate(jan_tavg = mean(tavg[month == 1]),
         feb_tavg = mean(tavg[month == 2]),
         mar_tavg = mean(tavg[month == 3]),
         apr_tavg = mean(tavg[month == 4]),
         may_tavg = mean(tavg[month == 5]),
         jun_tavg = mean(tavg[month == 6]),
         jul_tavg = mean(tavg[month == 7]),
         aug_tavg = mean(tavg[month == 8]),
         sep_tavg = mean(tavg[month == 9]),
         oct_tavg = mean(tavg[month == 10]),
         nov_tavg = mean(tavg[month == 11]),
         dec_tavg = mean(tavg[month == 12]))

# Create features for peak swe and the peak_swe/precip ratio
df_wy <- df_wy %>% 
  mutate(peak_swe=max(swe)) %>%
  mutate(swe_precip_ratio=peak_swe/sum(precip)) %>% 
  summarise_if(is.numeric, mean) %>% 
  ungroup()

# Reorder response variables first and remove any unwanted variables (manually?)
df_wy <- df_wy %>% 
  select(!!response_var, everything()) %>%
  select(-c(tavg, day, month, year, basinID, hillID, zoneID, patchID))

# Create data frame with only climate scenario 0
df_wy0 <- df_wy %>% 
  filter(clim==0) %>% 
  select(-c(clim, wy))

# Create data frame with only climate scenario 2
df_wy2 <- df_wy %>% 
  filter(clim==2) %>% 
  select(-c(clim, wy))
```

Now, we have two data tables representing the two climate scenarios.

# Data Description

Next, we want to get a summary description of the data set. This is crucial for many reasons:

1.  Shows early on in the process if any variables do not have the expected range, magnitude, class, etc.

2.  Provides information on data set characteristics that are important for decisions later on in the machine learning process.

Here, we display summary statistics for the base climate scenario data.

```{r describe_data}
summarize_data <- function(df) {
  df_name <- deparse(substitute(df))
  describe(df) %>% 
    select(vars, n, mean, sd, min, max, range) %>% 
    mutate(class = lapply(df, class)) %>% 
    kable(digits=4,
          align="r",
          caption=paste0("Summary of ", df_name)) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"))
}

summarize_data(df_wy0)
#summarize_data(df_wy2)
```

We can also look at the key study area characteristics. In this study, there are six study areas.

```{r describe_topos}
topos <- df_wy %>%
  filter(clim==0) %>% 
  select(c(stratumID, topo, elev, aspect, slope)) %>%
  group_by(stratumID, topo) %>% 
  summarize_if(is.numeric, mean) %>% 
  mutate(topo = case_when(topo == "M" ~ "Mid-Slope",
                          topo == "U" ~ "Upslope",
                          topo == "R" ~ "Riparian")) %>%
  dplyr::arrange(topo) %>%
  rename("Stratum"=stratumID,
         "Topo"=topo,
         "Elevation (m)"=elev,
         "Aspect (degrees CCW from East)"=aspect,
         "Slope (degrees)"=slope)

kable(topos, 
      digits=2, 
      caption="Summary of 6 Study Areas in Sagehen Creek", 
      align=c("l", rep("r", ncol(topos) - 1))) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# Remove Multicollinearity

Highly correlated predictor variables are not as much a concern in machine learning when creating a predictive model. However, for this process of assessing relative predictor variable importance, multicollinear variables have biased importance (Strobl et al. 2008). Therefore, these need to be handled prior to assessing feature importance.

## Identify and Remove Correlated Variables

First, we create data frames containing only the predictor variables to assist with the next steps.

```{r get_predictors}
# Find number of response variables specified
num_response <- length(response_var)

# Save data frames of predictor variables for first climate scenario
numericpredictors.df_wy0 <- df_wy0[,(num_response+1):ncol(df_wy0)] %>% select(where(is.numeric))
allpredictors.df_wy0 <- df_wy0[,(num_response+1):ncol(df_wy0)]
factorpredictors.df_wy0 <- df_wy0[,(num_response+1):ncol(df_wy0)] %>% select(where(is.factor))

# Save data frames of predictor variables for second climate scenario
numericpredictors.df_wy2 <- df_wy2[,(num_response+1):ncol(df_wy2)] %>% select(where(is.numeric))
allpredictors.df_wy2 <- df_wy2[,(num_response+1):ncol(df_wy2)]
factorpredictors.df_wy2 <- df_wy2[,(num_response+1):ncol(df_wy2)] %>% select(where(is.factor))
```

Next, we use Variance Inflation Factors (VIF) and Pearson Correlation Coefficients to remove variables with high multicollinearity. In the code below, an initial preference order of variables selected is determined using a preliminary random forest method. While highly correlated variables will not be in the correct order here, their relative importance should be reasonably accurate.

**Do we want to do this preliminary imp process for clim 2 as well?**

```{r}
# First climate scenario

# Find preliminary importance using random forest
imp <- rf(data=df_wy,
          dependent.variable.name=response_var,
          predictor.variable.names=colnames(allpredictors.df_wy0),
          verbose=FALSE)

# Set preference order based on variable importance
preference.order <- imp$importance$per.variable$variable

# Preference order can be determined manually for variables of interest:
#preference.order <- c("precip", "rz_storage", "trans", "evap")
```

```{r find_preference_order}
# First climate scenario

# Find preliminary importance using random forest
imp0 <- randomForest(formula=npp~.,
                     data=df_wy0,
                     replace=TRUE,
                     importance=TRUE,
                     keep.forest=TRUE,
                     keep.inbag=TRUE)

# Set preference order based on variable importance
preference.order <- varImp(imp0) %>% arrange(-Overall) %>% rownames()

# Preference order can be determined manually for variables of interest:
#preference.order <- c("precip", "rz_storage", "trans", "evap")
```

```{r create_correlation_functions}
# Remove variables based on VIF and Correlation thresholds
remove_vif <- function(predictors.df, vif.threshold=5) {
  variable.selection <- auto_vif(x=predictors.df,
                                 vif.threshold=vif.threshold,
                                 preference.order=preference.order) 
  
  return(variable.selection)
}

remove_cor <- function(predictors.df, cor.threshold=0.75) {
  variable.selection <- auto_cor(x=predictors.df,
                                 cor.threshold=cor.threshold,
                                 preference.order=preference.order) 
  
  return(variable.selection)
}
```

Thresholds for VIF and correlation can be set using the function below, with default values of 5 and 0.75, respectively.

```{r remove_multicollinearity, warning=FALSE}
# Create list of selected variables
wy0_vif <- remove_vif(predictors.df=allpredictors.df_wy0, vif.threshold=5)$selected.variables
wy0_cor <- remove_cor(predictors.df=allpredictors.df_wy0, cor.threshold=0.75)$selected.variables
wy0_select_variables <- unique(append(wy0_vif, wy0_cor))

wy2_vif <- remove_vif(predictors.df=allpredictors.df_wy2, vif.threshold=5)$selected.variables
wy2_cor <- remove_cor(predictors.df=allpredictors.df_wy2, cor.threshold=0.75)$selected.variables
wy2_select_variables <- unique(append(wy2_vif, wy2_cor))

# Remove numeric variables with multicollinearity
df_wy0_reduced <- df_wy0 %>% 
  select(c(all_of(response_var), colnames(factorpredictors.df_wy0), all_of(wy0_select_variables)))

df_wy2_reduced <- df_wy2 %>% 
  select(c(all_of(response_var), colnames(factorpredictors.df_wy2), all_of(wy2_select_variables)))
```

## Summary of Removed Variables

```{r}
# Create df with preliminary variable importance (and rank) and whether each was selected or removed
removed_importance0 <- imp$variable.importance %>% 
  data.frame() %>% 
  rownames_to_column("variable") %>%
  rename("importance" = ".") %>% 
  mutate("importance_rank" = rank(-importance)) %>% 
  mutate(selected = case_when(variable %in% wy0_select_variables ~ "selected",
                              !variable %in% wy0_select_variables ~ "removed")) %>% 
  relocate("selected", .after = "variable")

# creating df of VIFs
removed_vif0 <- vif(allpredictors.df_wy0)

# joining dfs to create summary table of removed and selected variables
removed_summary0 <- removed_importance0 %>% 
  left_join(removed_vif0, by = "variable")

# create df of only removed variables
removed_corr0 <- df_wy0[-which(names(df_wy0) %in% c(wy0_select_variables, group_cols))]

removed_imp0_plot <- ggplot(removed_summary0, aes(x = importance, y = reorder(variable, importance), fill = selected)) +
  geom_col() +
  labs(x = "Preliminary Importance", y = "Variable")

removed_vif0_plot <- ggplot(removed_summary0, aes(x = vif, y = reorder(variable, vif), fill = selected)) +
  geom_col() +
  labs(x = "Variable Inflation Factor", y = "Variable")

removed_pairs_plot <- GGally::ggpairs(removed_corr0)

removed_summary0 %>%
  select(-importance) %>% 
  kable(caption="Selected and removed variables",
                            format.args=list()) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))

removed_imp0_plot
removed_vif0_plot
removed_pairs_plot
```

# Feature Importance

Now, the data frames are adequately prepared to determine predictor feature importance. In this framework, we use the Random Forest because this method has been proven to be able to make predictions and assess feature importance with a high degree of accuracy in numerous applications including ecological analysis (Cutler et al. 2007; Prasad et al. 2006). Additionally, Random Forest requires relatively less hyper-parameter tuning than other common techniques.

### Hyper-Parameter Tuning

A primary advantage of using a random forest model is that there is relatively little hyper-parameter tuning needed. Below, we tune the `mtry` parameter, which determines the number of variables randomly sampled as candidates at each split. The default value for regression is p/3, where p is the number of predictor variables.

```{r tune_mtry_function}
set.seed(4326)

df_wy0_reduced <- as.data.frame(df_wy0_reduced)
df_wy2_reduced <- as.data.frame(df_wy2_reduced)

tune_rf_model <- function(df) {
  
  tuning <- vector(length = (ncol(df)-num_response))
  
  x=df[,-1]
  y=df[,1]
  
  for (i in 1:length(tuning)) {
    rf_tuning <- randomForest(x, y, mtry = i, ntree = 50)
    tuning[i] <- tail(rf_tuning$mse, 1)
  }
  
  return(tuning)
  
  # bestmtry <- match(min(tuning), tuning)
  # return(bestmtry)
}
```

```{r tune_rf_wy0}
mtry0 <- tune_rf_model(df_wy0_reduced)
bestmtry0 <- match(min(mtry0), mtry0)

ggplot(data=as.data.frame(mtry0), aes(x=1:length(mtry0), y=mtry0)) +
  geom_vline(xintercept=bestmtry0, linetype="dashed") +
  geom_point() +
  geom_line() +
  theme_light() +
  labs(x="mtry value",
       y="OOB Error",
       title=paste0("The best mtry value for ", deparse(substitute(df_wy0_reduced)), " is ", bestmtry0))
```

```{r tune_rf_wy2}
mtry2 <- tune_rf_model(df_wy2_reduced)
bestmtry2 <- match(min(mtry2), mtry2)

ggplot(data=as.data.frame(mtry2), aes(x=1:length(mtry2), y=mtry2)) +
  geom_vline(xintercept=bestmtry2, linetype="dashed") +
  geom_point() +
  geom_line() +
  theme_light() +
  labs(x="mtry value",
       y="OOB Error",
       title=paste0("The best mtry value for ", deparse(substitute(df_wy2_reduced)), " is ", bestmtry2))
```

### Random Forest Models

```{r get_random_forests}
set.seed(4326)
rf_wy0 <- randomForest(formula=npp~.,
                       data=df_wy0_reduced,
                       mtry=bestmtry0,
                       replace=TRUE,
                       importance=TRUE,
                       keep.forest=TRUE,
                       keep.inbag=TRUE)

set.seed(4326)
rf_wy2 <- randomForest(formula=npp~.,
                       data=df_wy2_reduced,
                       mtry=bestmtry2,
                       replace=TRUE,
                       importance=TRUE,
                       keep.forest=TRUE,
                       keep.inbag=TRUE)
```

Assessing feature importance is a complex task with many possible approaches. Tree based models like random forest offer convenient "split-improvement" measures, like mean increase in purity and minimum depth, which are intrinsically derived during model construction. However, these have been shown to be biased towards variables with many categories or large value ranges (Strobl et al. 2007). Permutation importance is another measure that is applied to many model types including random forest---this approach, however, has been shown to be biased towards correlated predictor variables (Strobl et al. 2008). Conditional permutation importance (CPI) is a measure that seeks to assess importance of each feature *conditional* on the other features in the model. This approach tends to offer less biased results and has been used to assess ecosystem dynamics (Strobl et al. 2008). Here we use a recent implementation of CPI developed by Strobl and Debeer (2020), via their `permimp` package.

```{r get_importance, warning=FALSE}
imp_wy0 <- permimp(rf_wy0, do_check=FALSE, progressBar=FALSE)

imp_wy2 <- permimp(rf_wy2, do_check=FALSE, progressBar=FALSE)
```

# Model Evaluation

The following section provides visualizations and statistics to evaluate the random forest performance.

First, simply calling the random forest shows a summary of the results, including percent variance explained.

```{r}
rf_wy0
```

The model for the base climate scenario explained 90.34% of variance in NPP.

```{r}
rf_wy2
```

The model for the +2 degree Celsius climate scenario explained 88.05% of variance in NPP.

Next, we can implement a permutation test cross-validation for the random forest models.

```{r}
allpredictors.df_wy0_reduced <- df_wy0_reduced[,(num_response+1):ncol(df_wy0_reduced)]

rf.crossValidation(x=rf_wy0, xdata=allpredictors.df_wy0_reduced, p=0.1, n=10)

```

The results above indicate that, through 10-fold cross validation with 10% of data withheld, the model for the base climate scenario data explains 89.68% of the variance in NPP. This result is 0.66% lower than our original fit % explained.

```{r}
allpredictors.df_wy2_reduced <- df_wy2_reduced[,(num_response+1):ncol(df_wy2_reduced)]

rf.crossValidation(x=rf_wy2, xdata=allpredictors.df_wy2_reduced, p=0.1, n=10)
```

The results above indicate that, through 10-fold cross validation with 10% of data withheld, the model for the +2 degree Celsius climate scenario data explains 86.99% of the variance in NPP. This result is 1.06% lower than our original fit % explained.

# Visualize Results

Now, we can visualize the results of the Random Forest feature selection.

```{r create_imp_table}
imp_to_table <- function(imp) {
  df_permimp <- imp$values %>%
    data.frame() %>% 
    rownames_to_column("Variable") %>% 
    rename("Importance"=".") %>% 
    mutate(Rank=rank(-Importance))
}

df_imp_wy0 <- imp_to_table(imp_wy0)
df_imp_wy2 <- imp_to_table(imp_wy2)

# rank_importance <- function(df_imp) {
#   df_name <- deparse(substitute(df_imp))
#   df_imp_wy0 %>% 
#   select(Variable, Rank) %>% 
#   arrange(Rank) %>% 
#   kable(align="r",
#         caption=paste0("Variable importance rank for ", df_name)) %>% 
#   kable_styling(bootstrap_options = c("striped", "hover"))
# }
# 
# rank_importance(df_imp_wy0)
# rank_importance(df_imp_wy2)
```

The following table shows the relative importance of predictor variables between the two climate scenarios.

```{r}
df_imp <- df_imp_wy0 %>% 
  full_join(df_imp_wy2, by="Variable") %>% 
  mutate(Diff = Rank.x-Rank.y) %>% 
  select(-c(Importance.x, Importance.y)) %>% 
  rename("Rank (0)" = Rank.x, 
         "Rank (2)" = Rank.y) %>% 
  arrange(`Rank (0)`)

df_imp %>% 
  kable(align="r",
        caption="Difference in variable importance for the two climate scenarios") %>% 
  kable_styling(bootstrap_options = c("striped", "hover")) %>% 
  column_spec(column=4,
              color=ifelse(is.na(df_imp$Diff), "grey",
                           ifelse(df_imp$Diff>0, "green", 
                                  ifelse(df_imp$Diff==0, "grey", "red"))))
```

We see that precipitation and rz_storage are the first and second most important predictors of NPP for both climate scenarios. The highest difference is evaporation - which is third important for the base climate scenario and sixth for the +2 degree Celsius warming scenario. This indicates that the relationship between NPP and evaporation has changed given warming, which could be investigated further. This process can be repeated for all other observations found in the table.

This same information can be plotted below. The variable importance plots also reveal relative magnitude of importance between variables. Note that this is somewhat of an artifact of the mechanics of a random forests model on not necessarily reflective of the actual relative importance of these variables in the natural world.

```{r plot_imp}
plot_imp <- function(imp_df) {
  df_name <- deparse(substitute(imp_df))
  ggplot(data=imp_df, aes(x=Importance, y=reorder(Variable, Importance))) +
    geom_col() +
    theme_light() +
    theme(legend.position = "none",
          axis.text.x = element_blank()) +
    labs(title=paste0("Variable Importance for \n", df_name),
         x="Importance",
         y="Variable")
}

wy0_plot <- plot_imp(df_imp_wy0)
wy2_plot <- plot_imp(df_imp_wy2)

wy0_plot + wy2_plot 
```

## Previewing relationships between important predictors and NPP

The variable importance values derived from our random forest model allow us to investigate intriguing relationships between variables that have been identified as having greater importance.

Here, we investigate the relationships of some of the most important identified variables with NPP as well as their interactions.

```{r message=FALSE}
# assigning first and second most important variables to objects
pred1_clim0 <- df_imp_wy0$Variable[df_imp_wy0$Rank == 1]
pred2_clim0 <- df_imp_wy0$Variable[df_imp_wy0$Rank == 2]
pred1_clim2 <- df_imp_wy2$Variable[df_imp_wy2$Rank == 1]
pred2_clim2 <- df_imp_wy2$Variable[df_imp_wy2$Rank == 2]

df0_pred_binned <- df_wy0 %>% 
  select(pred1_clim0, pred2_clim0, response_var) %>% 
  mutate(pred1 = df_wy0[[pred1_clim0]],
         pred2 = df_wy0[[pred2_clim0]]) %>% 
  mutate(bin = as.numeric(ntile(pred2, 4)))

df2_pred_binned <- df_wy2 %>% 
  select(pred1_clim2, pred2_clim2, response_var) %>% 
  mutate(pred1 = df_wy2[[pred1_clim2]],
         pred2 = df_wy2[[pred2_clim2]]) %>% 
  mutate(bin = as.numeric(ntile(pred2, 4)))
```

```{r}
ggplot(df0_pred_binned, aes(x = pred1, y = .data[[response_var]])) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(x = str_to_title(pred1_clim0), y = response_var,
       title = paste(pred1_clim0, "vs", response_var,
                     "in 0 degree climate scenario")) +
  theme_minimal()

ggplot(df0_pred_binned, aes(x = pred1, y = .data[[response_var]])) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap("bin") +
  labs(x = str_to_title(pred1_clim0), y = response_var,
       title = paste(pred1_clim0, "vs", response_var, "given different value classes of",
                     pred2_clim0, "\n in 0 degree climate scenario")) +
  theme_minimal()

ggplot(df2_pred_binned, aes(x = pred1, y = .data[[response_var]])) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap("bin") +
  labs(x = str_to_title(pred1_clim2), y = response_var,
       title = paste(pred1_clim0, "vs", response_var, "given different value classes of",
                     pred2_clim0, "\n in 2 degree climate scenario")) +
  theme_minimal()
```

Variable interactions can be investigated in many ways. The bivariate partial dependence plot below provides a graphical depiction of the marginal effect of two variables on predicted NPP from the random forest model.

```{r}
bivariate.partialDependence(x=rf_wy0,
                            pred.data=allpredictors.df_wy0_reduced,
                            v1="lai",
                            v2="peak_swe",
                            grid.size=15,
                            col.ramp=c("orange", "green"),
                            ncols=20)
```

Running this plot for both data sets can demonstrate how variable interactions shift given different climate scenarios.

```{r}
bivariate.partialDependence(x=rf_wy2,
                            pred.data=allpredictors.df_wy2_reduced,
                            v1="lai",
                            v2="peak_swe",
                            grid.size=15,
                            col.ramp=c("orange", "green"),
                            ncols=20)
```

Variable interactions between two predictors and the corresponding NPP predictions from the random forest model can be displayed as a grid of values.

Below, the x-axis represents values of `rz_storage` , y-axis represents values of `precip` , and the cell colors/values represent predicted values of `npp` , with red indicating high and blue indicating low. For this interaction, we can see that high values of precipitation and root-zone storage result in higher predictions of NPP. This is only true for root zone storage given high precipitation, while high precipitation at all levels of root zone storage predicts relatively higher NPP. This is another indicator that it is an important predictor.

```{r}
plot_predict_interaction(rf_wy0, df_wy0_reduced, 
                         "rz_storage", "precip",
                         grid=50) # smaller grid values reveal more general relationships
```

## Comparison with other measures of importance

The capstone team chose to use conditional permutation importance above as our main representation of importance. However there are other potential methods of determining relative importance from random forest models that are demonstrated below. Note that the interpretation of these measures, as discussed in the original justification of using the `permimp` package, is limited due to characteristics of RHESSys data. Namely, the presence of both categorical and continuous variables, differing number of classes in categorical variables, differing scales of numeric variables, and potential remaining multicollinearity all have greater potential to skew the results of the methods below.

The plot below presents the top 10 variables using % mean decrease in accuracy (%IncMSE) and mean decrease in node impurity (IncNodePurity). %IncMSE is computed from permuting the out-of-bag data, while IncNodePurity is the mean total decrease in node impurities from splitting on the given variable, as measured by the residual sum of squares.

```{r}
varImpPlot(rf_wy0, n.var=10)
```

The plot below visualizes the distribution of minimum depth from the random forest model. Low values, such as for precipitation below, indicate that many observations are divided into groups based on that variable.

```{r}
min_depth_frame <- min_depth_distribution(rf_wy0)

plot_min_depth_distribution(min_depth_frame,
                            mean_sample="relevant_trees")
```

# References

Cutler, D. Richard, Thomas C. Edwards, Karen H. Beard, Adele Cutler, Kyle T. Hess, Jacob Gibson, and Joshua J. Lawler. 2007. "Random Forests for Classification in Ecology." *Ecology* 88 (11): 2783--92. <https://doi.org/10.1890/07-0539.1>.

Debeer, Dries, and Carolin Strobl. 2020. "Conditional Permutation Importance Revisited." *BMC Bioinformatics* 21 (1): 307. <https://doi.org/10.1186/s12859-020-03622-2>.

Prasad, Anantha M., Louis R. Iverson, and Andy Liaw. 2006. "Newer Classification and Regression Tree Techniques: Bagging and Random Forests for Ecological Prediction." *Ecosystems* 9 (2): 181--99. <https://doi.org/10.1007/s10021-005-0054-1>.

Strobl, Carolin, Anne-Laure Boulesteix, Achim Zeileis, and Torsten Hothorn. 2007. "Bias in Random Forest Variable Importance Measures: Illustrations, Sources and a Solution." *BMC Bioinformatics* 8 (1): 25. <https://doi.org/10.1186/1471-2105-8-25>.

Strobl, Carolin, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. 2008. "Conditional Variable Importance for Random Forests." *BMC Bioinformatics* 9 (1): 307. <https://doi.org/10.1186/1471-2105-9-307>.
