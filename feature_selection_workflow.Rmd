---
title: "Feature Selection Framework"
author: "RHESSysML Capstone Group"
date: "2/5/2022"
output: 
  rmarkdown::html_document:
    theme: cerulean
---

# Introduction

The following R Markdown document describes the necessary steps to determine important relationships between predictor variables and a response variable in RHESSys model output. Specific code examples will be based on RHESSys model output from the Sagehen Creek Experimental Watershed in the Sierra Nevada, CA. The data set incorporates model parameter uncertainty, topographic spatial variability, and climate change scenarios. The data set and associated metadata can be accessed here: https://www.hydroshare.org/resource/2a31bd57b7e74c758b7857679ffbb4c5/. 

The following research question will be addressed in this process: **What are the most important predictors of Net Primary Productivity in differing climate scenarios?**

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(scipen=999)

## Standard packages
library(tidyverse)
library(here)
library(patchwork)
library(psych)
library(kableExtra)

## Machine Learning packages
library(caret)
library(spatialRF)
library(randomForest)
library(party)
library(partykit)
library(permimp)

#source("feature_selection.R")
```

# Data Preparation

## Load Data

```{r load_data, message=FALSE}
df <- read_csv(here("data", "sageres.csv")) 
```

## Clean and Aggregate Data

First, RHESSys output will need to be aggregated and cleaned based on specific research question. Possible changes include:

1. Changing the temporal resolution (i.e. Daily to Yearly measurements).

2. Converting variable units (i.e. Radians to Degrees).

3. Converting variable class (i.e. numeric/character to factor).

4. Creating derived variables (i.e. Peak SWE).

The code chunk below identifies the variables in our data set that will be converted to factors, variables that will be used to aggregate by water year, and the desired response variable.

```{r user_inputs}
## USER INPUTS
group_cols <- c("wy", "stratumID", "clim", "scen", "topo")
factor_vars <- c("wy", "stratumID", "clim", "scen", "topo")
response_var <- "npp"
```

Next, the necessary modifications to the data set are made. For this example, we are converting factor variables, aggregating by water year, changing `slope` and `aspect` from radians to degrees, and adding two derived variables: `peak_swe` and `swe_precip_ratio`. Since this data set contains two climate scenarios, we split these up into two data frames

TO DO: ADD DERIVED VARIABLES (SWE AND MONTHLY TEMPS)

```{r prepare_data}
## Convert categorical variables to factors
df[,factor_vars] <- lapply(df[,factor_vars], factor)

df_wy <- df %>% 
  ## Change aspect and splope from radians to degrees
  mutate(aspect=aspect*(180/pi),
         slope=slope*(180/pi)) %>% 
  group_by(across(all_of(group_cols))) %>% 
  summarise_if(is.numeric, mean) %>% 
  ungroup() %>% 
  ## Reorder response variables first
  select(!!response_var, everything()) %>% 
  ## Remove unwanted variables (manually?)
  select(-c(day, month, year, basinID, hillID, zoneID, patchID, wy))

## Create data frame with only climate scenario 0
df_wy0 <- df_wy %>% 
  filter(clim==0) %>% 
  select(-clim)

## Create data frame with only climate scenario 2
df_wy2 <- df_wy %>% 
  filter(clim==2) %>% 
  select(-clim)
```

Now, we have two data tables representing the two climate scenarios.

# Data Description

Next, we want to get a summary description of the data set. This is crucial for many reasons:

1. Shows early on in the process if any variables do not have the expected range, magnitude, class, etc.

2. Provides information on data set characteristics that are important for decisions later on in the machine learning process.

```{r describe_data}
summarize_data <- function(df) {
  df_name <- deparse(substitute(df))
  describe(df) %>% 
    select(vars, n, mean, sd, min, max, range) %>% 
    mutate(class = lapply(df, class)) %>% 
    kable(digits=4,
          align="r",
          caption=paste0("Summary of ", df_name)) %>% 
    kable_styling(bootstrap_options = c("striped", "hover"))
}

summarize_data(df_wy0)
summarize_data(df_wy2)
```

We can also look at the key study area characteristics. In this study, there are six study areas.

```{r describe_topos}
topos <- df_wy %>% 
  filter(clim==0) %>% 
  select(c(stratumID, topo, elev, aspect, slope, tmin, tavg, tmax, precip)) %>%
  ## Aggregate by year
  group_by(topo, stratumID, elev, aspect, slope) %>% 
  summarize(tmin = mean(tmin),
            tavg = mean(tavg),
            tmax = mean(tmax),
            precip = sum(precip)*1000) %>% 
  ## Aggregate by study area
  group_by(stratumID, topo, elev, aspect, slope) %>% 
  summarize(across(everything(), list(mean))) %>% 
  ungroup() %>% 
  rename_with(~str_remove(., "_1")) %>% 
  mutate(topo = case_when(topo == "M" ~ "Mid-Slope",
                          topo == "U" ~ "Upslope",
                          topo == "R" ~ "Riparian")) %>% 
  dplyr::arrange(topo) %>% 
  rename("Stratum"=stratumID,
         "Topo"=topo,
         "Elevation (m)"=elev,
         "Aspect (degrees CCW from East)"=aspect,
         "Slope (degrees)"=slope,
         "Min Temp.(C)"=tmin,
         "Mean Temp.(C)"=tavg,
         "Max Temp. (C)"=tmax,
         "Precipitation (mm)"=precip)

kable(topos, 
      digits=2, 
      caption="Summary of 6 Study Areas in Sagehen Creek", 
      align=c("l", rep("r", ncol(topos) - 1))) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
```


# Remove Multicollinearity

Highly correlated predictor variables are not as much a concern in machine learning when creating a predictive model (ADD SOURCE). However, for this process of assessing relative predictor variable importance, multicollinear variables have biased importance (ADD SOURCE). Therefore, these need to be handled prior to assessing feature importance.

TO DO: ADD SOURCES FOR THIS SECTION

## Identify and Remove Correlated Variables

First, we create data frames containing only the predictor variables to assist with the next steps.

```{r get_predictors}
## Find number of response variables specified
num_response <- length(response_var)

## Save data frames of predictor variables for first climate scenario
numericpredictors.df_wy0 <- df_wy0[,(num_response+1):ncol(df_wy0)] %>% select(where(is.numeric))
allpredictors.df_wy0 <- df_wy0[,(num_response+1):ncol(df_wy0)]
factorpredictors.df_wy0 <- df_wy0[,(num_response+1):ncol(df_wy0)] %>% select(where(is.factor))

## Save data frames of predictor variables for second climate scenario
numericpredictors.df_wy2 <- df_wy2[,(num_response+1):ncol(df_wy2)] %>% select(where(is.numeric))
allpredictors.df_wy2 <- df_wy2[,(num_response+1):ncol(df_wy2)]
factorpredictors.df_wy2 <- df_wy2[,(num_response+1):ncol(df_wy2)] %>% select(where(is.factor))
```

Next, we use Variance Inflation Factors (VIF) and Pearson Correlation Coefficients to remove variables with high multicollinearity ([Source](## Reference: https://gist.github.com/BlasBenito/768a45951a3d0c5ba355f52be94b05de)). In the code below, an initial preference order of variables selected is determined using a preliminary random forest method. While highly correlated variables will not be in the correct order here, their **relative** importance should be accurate.

```{r create_correlation_function}
## First climate scenario

## Find preliminary importance using random forest
imp <- rf(data=df_wy,
          dependent.variable.name=response_var,
          predictor.variable.names=colnames(allpredictors.df_wy0),
          verbose=FALSE)

## Set preference order based on variable importance.
preference.order <- imp$importance$per.variable$variable

## Preference order can be determined manually for variables of interest:
#preference.order <- c("precip", "rz_storage", "trans", "evap")

## Remove variables based on VIF and Correlation thresholds
remove_multicollinearity <- function(vif.threshold=5, cor.threshold=0.75, predictors.df) {
  variable.selection <- auto_vif(x=predictors.df,
                                 vif.threshold=vif.threshold,
                                 preference.order=preference.order) %>% 
    auto_cor(cor.threshold=cor.threshold,
             preference.order=preference.order)
  
  variable.selection$selected.variables
}
```

Thresholds for VIF and correlation can be set using the function below. For information on how to select appropriate thresholds, see here (ADD SOURCE)

TO DO: ADD SOURCE FOR SELECTING THRESHOLDS

```{r remove_multicollinearity, warning=FALSE}
## Create list of selected variables
wy0_select_variables <- remove_multicollinearity(vif.threshold=5, cor.threshold=0.75, predictors.df=allpredictors.df_wy0)
wy2_select_variables <- remove_multicollinearity(vif.threshold=5, cor.threshold=0.75, predictors.df=allpredictors.df_wy2)

## Remove numeric variables with multicollinearity
df_wy0_reduced <- df_wy0 %>% 
  select(c(all_of(response_var), colnames(factorpredictors.df_wy0), all_of(wy0_select_variables)))

df_wy2_reduced <- df_wy2 %>% 
  select(c(all_of(response_var), colnames(factorpredictors.df_wy2), all_of(wy2_select_variables)))
```

## Summary of Removed Variables

TO DO: CREATE SUMMARY AND VISUALIZATIONS OF VARIABLES REMOVED. use warning messages for guidance.

# Feature Importance

Now, the data frames are adequately prepared to determine predictor feature importance. In this framework, we use the Random Forest method because ADD DESCRIPTION AND SOURCE HERE. Additionally, we use this package/function because ADD DESCRIPTION AND SOURCE HERE. All decisions should be explained/explored.

TO DO: ADD DESCRIPTIONS AND SOURCES TO THIS SECTION

TO DO: ADD MORE RANDOM FOREST MODELS AND IMPORTANCE METRICS

```{r get_random_forests}
## Random Forest package with replace=FALSE
rf_wy0 <- randomForest(formula=npp~.,
                       data=df_wy0_reduced,
                       replace=TRUE,
                       importance=TRUE)

## Random Forest package with replace=FALSE
rf_wy2 <- randomForest(formula=npp~.,
                       data=df_wy2_reduced,
                       replace=TRUE,
                       importance=TRUE)
```

Lastly, we measure variable importance using ADD DESCRIPTION AND SOURCE HERE. All decisions should be explained/explored.

TO DO: ADD DESCRIPTIONS AND SOURCES TO THIS SECTION

```{r get_importance, warning=FALSE}
imp_wy0 <- permimp(rf_wy0, 
                   #conditional=TRUE,
                   do_check=FALSE, progressBar=FALSE)

imp_wy2 <- permimp(rf_wy2, 
                   #conditional=TRUE,
                   do_check=FALSE, progressBar=FALSE)
```

TO DO: ADD CROSS VALIDATION (10-Fold)

# Visualize Results

Now, we can visualize the results of the Random Forest feature selection.

```{r create_imp_table}
imp_to_table <- function(imp) {
  df_permimp <- imp$values %>%
    data.frame() %>% 
    rownames_to_column("Variable") %>% 
    rename("Importance"=".") %>% 
    mutate(Rank=rank(-Importance))
}

df_imp_wy0 <- imp_to_table(imp_wy0)
df_imp_wy2 <- imp_to_table(imp_wy2)

rank_importance <- function(df_imp) {
  df_name <- deparse(substitute(df_imp))
  df_imp_wy0 %>% 
  select(Variable, Rank) %>% 
  arrange(Rank) %>% 
  kable(align="r",
        caption=paste0("Variable importance rank for ", df_name)) %>% 
  kable_styling(bootstrap_options = c("striped", "hover"))
}

rank_importance(df_imp_wy0)
rank_importance(df_imp_wy2)
```

```{r plot_imp}
plot_imp <- function(imp_df) {
  df_name <- deparse(substitute(imp_df))
  ggplot(data=imp_df, aes(x=Importance, y=reorder(Variable, Importance), fill=Variable)) +
    geom_col() +
    theme_light() +
    theme(legend.position = "none",
          axis.text.x = element_blank()) +
    labs(title=paste0("Variable Importance for ", df_name),
         x="Importance",
         y="Variable")
}

wy0_plot <- plot_imp(df_imp_wy0)
wy2_plot <- plot_imp(df_imp_wy2)

wy0_plot
wy2_plot
```

TO DO: INCLUDE MORE VISUALIZATIONS, INCLUDING CV RESULTS, PLOTS OF IMPORTANT VARIABLE RELATIONSHIPS, ETC.

TO DO: INCLUDE VISUALIZATIONS COMPARING RF MODELS, DIFFERENT IMPORTANCE MEASURES

OVERALL TO DO: TEST, CHECK, AND CORRECT ALL INCORRECT ASSUMPTIONS, CHOICES, ETC. THAT LEAD TO INCORRECT RESULTS OR UNDESIRABLE WORKFLOW